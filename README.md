# Online Sequential Optimization

Course from O. Wintenberger for Master M2A @ Sorbonne Universit√©.   
Website: http://wintenberger.fr/ens.html

## Implemented Algorithms
- Gradient Descent, Constrained Gradient Descent 
- Stochastic Gradient Descent, Constrained SGD
- Regularized Follow the Leader (RFTL) variants: Stochastic Mirror Descent (SMD), Stochatic Exponential Gradient (SEG), Adaptative Gradient (AdaGrad)
- Bandits: SREG, SBEG
- ADAM variant: Adam, Projected Adam, AdaMax, Adam Temporal, Adamax Temporal

## Contributors
Lise Le Bodec, Paul Liautaud, Nicolas Olivain
