{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Convex Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we present the different algorithm implemented in this repository and compare them. Hyperparameters can be tuned globally for all the algorithms at the same time or individually, in each algorithm's own cell. This setup allows quick and easy comparison and can be used to experiment interractively with the different algoritms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib as Path\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Algorithms.Adam import adaMax, adaMaxTemporal, adam, adamP, adamTemporal, adamproj\n",
    "from Algorithms.Explo import sbeg, sreg\n",
    "from Algorithms.GD import GradientDescent, projected_gd\n",
    "from Algorithms.SGD import sgd, projected_sgd\n",
    "from Algorithms.RFTL import adagrad, seg, smd\n",
    "from Algorithms.ONS import ons\n",
    "from Models.LinearSVM import LinearSVM\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Parameters\n",
    "The default parameters for the algorithms. It can be particularized later in each specific algorithm block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "lr = 0.1\n",
    "nepoch = 1\n",
    "lbd = 1/3\n",
    "z = 100\n",
    "gamma = 1/8\n",
    "verbose = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "We start with loading and normalizing the data. We only conduct binary classification so we label images of zeros from MNIST as 1 and every other labels as -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv('mnist_train.csv', sep=',', header=None)   # Reading\n",
    "# Extract data\n",
    "train_data = mnist_train.values[:, 1:]\n",
    "# Normalize data\n",
    "train_data = train_data / np.max(train_data)\n",
    "train_data = np.c_[train_data, np.ones(train_data.shape[0])]         # Add intercept\n",
    "# Extract labels\n",
    "train_labels = mnist_train.values[:, 0]\n",
    "# if labels is not 0 => -1 (Convention chosen)\n",
    "train_labels[np.where(train_labels != 0)] = -1\n",
    "# if label is 0 ==> 1\n",
    "train_labels[np.where(train_labels == 0)] = 1\n",
    "\n",
    "mnist_test = pd.read_csv('mnist_test.csv', sep=',', header=None)\n",
    "test_data = mnist_test.values[:, 1:]\n",
    "test_data = test_data / np.max(test_data)\n",
    "test_data = np.c_[test_data, np.ones(test_data.shape[0])]\n",
    "test_labels = mnist_test.values[:, 0]\n",
    "test_labels[np.where(test_labels != 0)] = -1\n",
    "test_labels[np.where(test_labels == 0)] = 1\n",
    "\n",
    "time_dict = {}\n",
    "\n",
    "n, m = train_data.shape\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------GD----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "GDloss, wts = GradientDescent(model, train_data, train_labels, nepoch, lbd, verbose, lr)\n",
    "time_dict['gd'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "GDacc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, Unconstrained GD algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, GDloss[-1], GDacc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), GDloss, label = 'gd')\n",
    "GDaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(GDaccuracies, label = 'gd')\n",
    "GDerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(GDerrors, label = 'gd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Gradient Descend\n",
    "Gradient Descent with projection on $B_1(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------c_GD - z=\"+str(z)+\"----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "GDprojloss, wts = projected_gd(model, train_data, train_labels, nepoch, lbd, z, verbose, lr)\n",
    "time_dict['c_gd z='+str(z)] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "GDacc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, constrained GD (radius {:3d}) algorithm has a losof {:1.6f} and accuracy {:1.6f}'.format(nepoch, z, GDprojloss[-1], GDacc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), GDprojloss, label = 'c_gd z='+str(z))\n",
    "GDprojaccuracies = compute_accuracies(wts, test_data, test_labels, average=False)  # no average for gd\n",
    "ax[1].plot(GDprojaccuracies, label = 'c_gd z='+str(z))\n",
    "GDprojerrors = compute_errors(wts, test_data, test_labels, average=False)\n",
    "ax[2].plot(GDprojerrors,label = 'c_gd z='+str(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------SGD----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "SGDloss, wts = sgd(model, train_data, train_labels, nepoch, lbd, verbose, lr)\n",
    "time_dict['sgd'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, Unconstrained SGD algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, SGDloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), SGDloss, label = 'sgd')\n",
    "SGDaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(SGDaccuracies, label = 'sgd')\n",
    "SGDerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(SGDerrors, label = 'sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Stochastic Gradient Descent\n",
    "Stochastic Gradient Descent with projection on $B_1(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------c_SGD - z=\" + str(z)+\"----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "SGDprojloss, wts = projected_sgd(model, train_data, train_labels, nepoch, lbd, z, verbose, lr)\n",
    "time_dict['c_sgd z='+str(z)] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, constrained SGD (radius {:3d}) algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, z, SGDprojloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), SGDprojloss, label = 'c_sgd z='+str(z))\n",
    "SGDprojaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(SGDprojaccuracies, label = 'c_sgd z='+str(z))\n",
    "SGDprojerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(SGDprojerrors, label = 'c_sgd z='+str(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Miror Descent (SMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "SMDprojloss, wts = smd(model, train_data, train_labels, nepoch, lbd, z, lr, verbose)\n",
    "time_dict['smd'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, constrained SMD algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, SMDprojloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), SMDprojloss, label = 'smd')\n",
    "SMDprojaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(SMDprojaccuracies, label = 'smd')\n",
    "SMDprojerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(SMDprojerrors, label = 'smd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Exponential Gradient (SEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------SMD----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "SEGloss, wts = seg(model, train_data, train_labels, nepoch, lbd, z, lr, verbose)\n",
    "time_dict['seg'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, constrained SEG algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, SEGloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), SEGloss, label = 'seg')\n",
    "SEGaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(SEGaccuracies, label = 'seg')\n",
    "SEGerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(SEGerrors, label = 'seg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptative Gradient (Adagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------Adagrad - z=\" + str(z)+\"----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "Adagradloss, wts = adagrad(model, train_data, train_labels, nepoch, lbd, z, verbose)\n",
    "time_dict['adagrad'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, constrained Adagrad (radius {:3d}) algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, z, Adagradloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), Adagradloss, label = 'adagrad z='+str(z))\n",
    "Adagradaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(Adagradaccuracies, label = 'adagrad z='+str(z))\n",
    "Adagraderrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(Adagraderrors,  label = 'adagrad z='+str(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Newton Step (ONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------ONS - z=\" + str(z) + \"----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "ONSloss, wts = ons(model, train_data, train_labels, nepoch, lbd, gamma, z, verbose)\n",
    "time_dict['ons'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, ONS (radius {:3d} algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, z, ONSloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), ONSloss,  label = 'ons z='+str(z))\n",
    "ONSaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(ONSaccuracies,  label = 'ons z='+str(z))\n",
    "ONSerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(ONSerrors,  label = 'ons z='+str(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SREG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------SREG - z=\" + str(z) + \"----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "SREGloss, wts = sreg(model, train_data, train_labels, nepoch, lbd, z, lr, verbose)\n",
    "time_dict['sreg'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, constrained SREG (radius {:3d}) algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, z, SREGloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), SREGloss, label = 'sreg z='+str(z))\n",
    "SREGaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(SREGaccuracies, label = 'sreg z='+str(z))\n",
    "SREGerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(SREGerrors, label = 'sreg z='+str(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------SBEG - z=\" + str(z) + \"----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "SBEGloss, wts = sbeg(model, train_data, train_labels, nepoch, lbd, z, lr, verbose)\n",
    "time_dict['sbeg'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, constrained SBEG algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, SBEGloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), SBEGloss, label = 'sbeg z='+str(z))\n",
    "SBEGaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(SBEGaccuracies, label = 'sbeg z='+str(z))\n",
    "SBEGerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(SBEGerrors, label = 'sbeg z='+str(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------Adam - z=\" + str(z) + \"----------- \\n\")\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "Adamloss, wts = adam(model, train_data, train_labels, lr, nepoch, lbd, [0.9, 0.999], verbose)\n",
    "time_dict['adam'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, adam algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, Adamloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), Adamloss, label = 'adam z='+str(z))\n",
    "Adamaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(Adamaccuracies, label = 'adam z='+str(z))\n",
    "Adamerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(Adamerrors, label = 'adam z='+str(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Fixed LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "AdamLRloss, wts = adam(model, train_data, train_labels, lr, nepoch, lbd, [0.9, 0.999], verbose, adaptative_lr=False)\n",
    "time_dict['adam_fixlr'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, adam with fixed lr algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, AdamLRloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), AdamLRloss, label = 'adam_fixlr')\n",
    "AdamLRaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(AdamLRaccuracies, label = 'adam_fixlr')\n",
    "AdamLRerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(AdamLRerrors, label = 'adam_fixlr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "AdamProjloss, wts = adamproj(model, train_data, train_labels, lr, nepoch, lbd, z, [0.9, 0.999], verbose)\n",
    "time_dict['adamproj'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, projected adam algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, AdamProjloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), AdamProjloss, label = 'adamproj')\n",
    "AdamProjaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(AdamProjaccuracies, label = 'adamproj')\n",
    "AdamProjerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(AdamProjerrors, label = 'adamproj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 3\n",
    "\n",
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "AdamPloss, wts = adamP(model, train_data, train_labels, lr, nepoch, lbd, [0.9, 0.999], p, verbose)\n",
    "time_dict['adamp'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, adam with norm L{:3d} algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, p, AdamPloss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), AdamPloss, label = 'adamp')\n",
    "AdamPaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(AdamPaccuracies, label = 'adamp')\n",
    "AdamPerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(AdamPerrors, label = 'adamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "AdamTemploss, wts = adamTemporal(model, train_data, train_labels, lr, nepoch, lbd, [0.9, 0.999], verbose)\n",
    "time_dict['adamtemp'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, adam with temporal averaging algorithm has a loss of{:1.6f} and accuracy {:1.6f}'.format(nepoch, AdamTemploss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), AdamTemploss, label = 'adamtemp')\n",
    "AdamTempaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(AdamTempaccuracies, label = 'adamtemp')\n",
    "AdamTemperrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(AdamTemperrors, label = 'adamtemp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "AdaMaxLoss, wts = adaMax(model, train_data, train_labels, lr, nepoch, lbd, [0.9, 0.999], verbose)\n",
    "time_dict['adamax'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, AdaMax algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, AdaMaxLoss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), AdaMaxLoss, label = 'adamax')\n",
    "AdaMaxaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(AdaMaxaccuracies, label = 'adamax')\n",
    "AdaMaxerrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(AdaMaxerrors, label = 'adamax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adamax Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVM(m)\n",
    "tic = time.time()\n",
    "\n",
    "AdaMaxTempLoss, wts = adaMaxTemporal(model, train_data, train_labels, lr, nepoch, lbd, [0.9, 0.999], verbose)\n",
    "time_dict['adamaxtemp'] = (time.time() - tic)\n",
    "pred_test_labels = model.predict(test_data)\n",
    "acc = accuracy(test_labels, pred_test_labels)\n",
    "print('After {:3d} epoch, AdaMax with temporal averaging algorithm has a loss of {:1.6f} and accuracy {:1.6f}'.format(nepoch, AdaMaxTempLoss[-1], acc))\n",
    "\n",
    "ax[0].plot(np.arange(nepoch), AdaMaxTempLoss, label = 'adamaxtemp')\n",
    "AdaMaxTempaccuracies = compute_accuracies(wts, test_data, test_labels)\n",
    "ax[1].plot(AdaMaxTempaccuracies, label = 'adamaxtemp')\n",
    "AdaMaxTemperrors = compute_errors(wts, test_data, test_labels)\n",
    "ax[2].plot(AdaMaxTemperrors, label = 'adamaxtemp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log scale\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_yscale('logit')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_yscale('logit')\n",
    "ax[2].set_xscale('log')\n",
    "ax[2].set_yscale('logit')\n",
    "\n",
    "# legend\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "ax[0].set_title('Loss')\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[2].set_title('Error')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[2].set_xlabel('Epochs')\n",
    "\n",
    "\n",
    "plt.savefig('LossAccuraciesErrors.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "keys = list(time_dict.keys())\n",
    "sns.barplot(x=keys, y=[time_dict[k]*20 for k in keys])\n",
    "plt.savefig('execution_time.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "353c1c6c9e5e4b73b8eb17fc061f4010f186933b76124145d0eb3f80c10b91f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
